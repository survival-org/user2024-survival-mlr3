[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Intro to Machine Learning for Survival Analysis with mlr3",
    "section": "",
    "text": "Teaching Aims\n\n\n\n\nUnderstand how {mlr3} is structured\nAccess learners and (built-in) tasks\n\n\n\nTo get started, we load {mlr3verse}, which will load various packages from the {mlr3} ecosystem:\n\n\nCode\nlibrary(mlr3verse)\n\n\n{mlr3} ships with wrappers for many commonly used machine learning algorithms (“learners”).\nWe can access the list of available learners using the mlr_learners dictionary:\n\n\nCode\nsample(mlr_learners$keys(), 10)\n\n\n [1] \"regr.decision_table\" \"regr.cv_glmnet\"      \"clust.cobweb\"       \n [4] \"clust.hclust\"        \"classif.cv_glmnet\"   \"regr.kknn\"          \n [7] \"classif.LMT\"         \"classif.JRip\"        \"clust.pam\"          \n[10] \"regr.ranger\"        \n\n\nOne example:\n\n\nCode\nlrn(\"classif.ranger\")\n\n\n&lt;LearnerClassifRanger:classif.ranger&gt;: Random Forest\n* Model: -\n* Parameters: num.threads=1\n* Packages: mlr3, mlr3learners, ranger\n* Predict Types:  [response], prob\n* Feature Types: logical, integer, numeric, character, factor, ordered\n* Properties: hotstart_backward, importance, multiclass, oob_error,\n  twoclass, weights\n\n\n\n\n\n\n\n\nNote\n\n\n\nUse lrn(\"classif.ranger\")$help() to view the help page, with links to documentation for parameters and other information about the wrapped learner.\n\n\nBuilt-in tasks can be accessed using the mlr_tasks dictionary:\n\n\nCode\nhead(as.data.table(mlr_tasks)[, list(key, label, task_type, nrow, ncol, properties)])\n\n\nKey: &lt;key&gt;\n              key                     label task_type  nrow  ncol properties\n           &lt;char&gt;                    &lt;char&gt;    &lt;char&gt; &lt;int&gt; &lt;int&gt;     &lt;list&gt;\n1:   ames_housing          Ames House Sales      regr  2930    82           \n2:   bike_sharing       Bike Sharing Demand      regr 17379    14           \n3: boston_housing     Boston Housing Prices      regr   506    18           \n4:  breast_cancer   Wisconsin Breast Cancer   classif   683    10   twoclass\n5:  german_credit             German Credit   classif  1000    21   twoclass\n6:           ilpd Indian Liver Patient Data   classif   583    11   twoclass\n\n\nOne example:\n\n\nCode\ntsk(\"penguins_simple\")\n\n\n&lt;TaskClassif:penguins&gt; (333 x 11): Simplified Palmer Penguins\n* Target: species\n* Properties: multiclass\n* Features (10):\n  - dbl (7): bill_depth, bill_length, island.Biscoe, island.Dream,\n    island.Torgersen, sex.female, sex.male\n  - int (3): body_mass, flipper_length, year\n\n\n\n\n\n\n\n\nNote\n\n\n\nTasks encapsulate a data source (typically a data.table) and additional information regarding which variables are considered features and target. Tasks can also specify additional properties such as stratification, which we will see later."
  },
  {
    "objectID": "index.html#mlr3-basics",
    "href": "index.html#mlr3-basics",
    "title": "Intro to Machine Learning for Survival Analysis with mlr3",
    "section": "",
    "text": "Teaching Aims\n\n\n\n\nUnderstand how {mlr3} is structured\nAccess learners and (built-in) tasks\n\n\n\nTo get started, we load {mlr3verse}, which will load various packages from the {mlr3} ecosystem:\n\n\nCode\nlibrary(mlr3verse)\n\n\n{mlr3} ships with wrappers for many commonly used machine learning algorithms (“learners”).\nWe can access the list of available learners using the mlr_learners dictionary:\n\n\nCode\nsample(mlr_learners$keys(), 10)\n\n\n [1] \"regr.decision_table\" \"regr.cv_glmnet\"      \"clust.cobweb\"       \n [4] \"clust.hclust\"        \"classif.cv_glmnet\"   \"regr.kknn\"          \n [7] \"classif.LMT\"         \"classif.JRip\"        \"clust.pam\"          \n[10] \"regr.ranger\"        \n\n\nOne example:\n\n\nCode\nlrn(\"classif.ranger\")\n\n\n&lt;LearnerClassifRanger:classif.ranger&gt;: Random Forest\n* Model: -\n* Parameters: num.threads=1\n* Packages: mlr3, mlr3learners, ranger\n* Predict Types:  [response], prob\n* Feature Types: logical, integer, numeric, character, factor, ordered\n* Properties: hotstart_backward, importance, multiclass, oob_error,\n  twoclass, weights\n\n\n\n\n\n\n\n\nNote\n\n\n\nUse lrn(\"classif.ranger\")$help() to view the help page, with links to documentation for parameters and other information about the wrapped learner.\n\n\nBuilt-in tasks can be accessed using the mlr_tasks dictionary:\n\n\nCode\nhead(as.data.table(mlr_tasks)[, list(key, label, task_type, nrow, ncol, properties)])\n\n\nKey: &lt;key&gt;\n              key                     label task_type  nrow  ncol properties\n           &lt;char&gt;                    &lt;char&gt;    &lt;char&gt; &lt;int&gt; &lt;int&gt;     &lt;list&gt;\n1:   ames_housing          Ames House Sales      regr  2930    82           \n2:   bike_sharing       Bike Sharing Demand      regr 17379    14           \n3: boston_housing     Boston Housing Prices      regr   506    18           \n4:  breast_cancer   Wisconsin Breast Cancer   classif   683    10   twoclass\n5:  german_credit             German Credit   classif  1000    21   twoclass\n6:           ilpd Indian Liver Patient Data   classif   583    11   twoclass\n\n\nOne example:\n\n\nCode\ntsk(\"penguins_simple\")\n\n\n&lt;TaskClassif:penguins&gt; (333 x 11): Simplified Palmer Penguins\n* Target: species\n* Properties: multiclass\n* Features (10):\n  - dbl (7): bill_depth, bill_length, island.Biscoe, island.Dream,\n    island.Torgersen, sex.female, sex.male\n  - int (3): body_mass, flipper_length, year\n\n\n\n\n\n\n\n\nNote\n\n\n\nTasks encapsulate a data source (typically a data.table) and additional information regarding which variables are considered features and target. Tasks can also specify additional properties such as stratification, which we will see later."
  },
  {
    "objectID": "index.html#example-train-predict-evaluate",
    "href": "index.html#example-train-predict-evaluate",
    "title": "Intro to Machine Learning for Survival Analysis with mlr3",
    "section": "Example: Train-Predict-Evaluate",
    "text": "Example: Train-Predict-Evaluate\n\n\n\n\n\n\nTeaching Aims\n\n\n\n\nPerform a simple train-predict-evaluate step\nUse built-in classification task and learner\n\n\n\nThe below code snippet trains a random forest model on the penguins_simple task (a simplified version of the palmerpenguins dataset, but without missing values) and evaluates the model’s performance using the classification error metric:\n\n\nCode\ntask = tsk(\"penguins_simple\")\nlearner = lrn(\"classif.ranger\", num.trees = 10)\n\npart = partition(task, ratio = 0.8) # by default stratifies on the target column\n\nlearner$train(task, row_ids = part$train)\npreds = learner$predict(task, row_ids = part$test)\npreds$score(msr(\"classif.ce\"))\n\n\nclassif.ce \n         0"
  },
  {
    "objectID": "index.html#mlr3proba-basics",
    "href": "index.html#mlr3proba-basics",
    "title": "Intro to Machine Learning for Survival Analysis with mlr3",
    "section": "mlr3proba: Basics",
    "text": "mlr3proba: Basics\n\n\n\n\n\n\nTeaching Aims\n\n\n\n\nUnderstand survival tasks and how they differ from regression/classification\nKnow how to conduct basic modeling with {mlr3proba}\nPrediction types\nSurvival measures\n\n\n\n{mlr3proba} extends {mlr3} with survival analysis capabilities.\n\n\n\n\n\n\nImportant\n\n\n\nAs of now, {mlr3proba} is not on CRAN, but you can install it from GitHub or r-universe. More info is also available on the respective mlr3 book chapter.\n\n\n\nSurvival Tasks\nWe’ll start by using the built-in lung dataset, which is a survival task with 7 features and 168 observations:\n\n\nCode\nlibrary(mlr3proba)\ntask = tsk(\"lung\")\n\ntask\n\n\n&lt;TaskSurv:lung&gt; (168 x 9): Lung Cancer\n* Target: time, status\n* Properties: -\n* Features (7):\n  - int (6): age, meal.cal, pat.karno, ph.ecog, ph.karno, wt.loss\n  - fct (1): sex\n\n\nSee online reference to useful methods offered by the main TaskSurv class. Some examples:\nTarget Surv object from {survival} (+ denotes censored observation):\n\n\nCode\nhead(task$truth())\n\n\n[1]  455   210  1022+  310   361   218 \n\n\nProportion of censored observations:\n\n\nCode\ntask$cens_prop()\n\n\n[1] 0.2797619\n\n\nDoes the data satisfy the proportional hazards assumption? Get the p-value from the Grambsch-Therneau test (see ?survival::cox.zph):\n\n\nCode\ntask$prop_haz() # barely, p &gt; 0.05 =&gt; PH\n\n\n[1] 0.0608371\n\n\nUsing the autoplot() function from {ggplot2}, we get the Kaplan-Meier curve:\n\n\nCode\nlibrary(ggplot2)\nautoplot(task) +\n  labs(title = \"Lung dataset: Kaplan-Meier curve\")\n\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\n\n\n\n\n\n\n\n\nTasks shipped with {mlr3proba}:\n\n\nCode\nas.data.table(mlr_tasks)[task_type == \"surv\", list(key, label, nrow, ncol)]\n\n\nKey: &lt;key&gt;\n             key                       label  nrow  ncol\n          &lt;char&gt;                      &lt;char&gt; &lt;int&gt; &lt;int&gt;\n 1:         actg                    ACTG 320  1151    13\n 2:         gbcs        German Breast Cancer   686    10\n 3:         gbsg        German Breast Cancer   686    10\n 4:        grace                  GRACE 1000  1000     8\n 5:         lung                 Lung Cancer   168     9\n 6:         mgus                        MGUS   176     9\n 7:          pbc Primary Biliary Cholangitis   276    19\n 8:         rats                        Rats   300     5\n 9: unemployment       Unemployment Duration  3343     6\n10:      veteran                     Veteran   137     8\n11:         whas      Worcester Heart Attack   481    11\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nUse as_task_surv() to convert your own datasets to a TaskSurv object\nTry tsk(\"lung\")$help() to get more info about the dataset and pre-processing applied\n\n\n\n\n\nCoxPH learner\nThe classical Cox Proportional Hazards model:\n\n\nCode\ncox = lrn(\"surv.coxph\")\ncox\n\n\n&lt;LearnerSurvCoxPH:surv.coxph&gt;: Cox Proportional Hazards\n* Model: -\n* Parameters: list()\n* Packages: mlr3, mlr3proba, survival, distr6\n* Predict Types:  [crank], distr, lp\n* Feature Types: logical, integer, numeric, factor\n* Properties: weights\n\n\nTrain the cox model and access the fit object from the {survival} package:\n\n\nCode\nset.seed(42)\npart = partition(task, ratio = 0.8) # by default, stratification is on `status` variable\ncox$train(task, row_ids = part$train)\n\ncox$model\n\n\nCall:\nsurvival::coxph(formula = task$formula(), data = task$data(), \n    x = TRUE)\n\n                coef  exp(coef)   se(coef)      z      p\nage        1.341e-02  1.013e+00  1.258e-02  1.066 0.2864\nmeal.cal  -5.007e-05  9.999e-01  2.903e-04 -0.172 0.8631\npat.karno -2.142e-02  9.788e-01  9.055e-03 -2.366 0.0180\nph.ecog    5.936e-01  1.811e+00  2.500e-01  2.375 0.0176\nph.karno   2.541e-02  1.026e+00  1.263e-02  2.011 0.0443\nsexm       4.510e-01  1.570e+00  2.298e-01  1.962 0.0497\nwt.loss   -1.500e-02  9.851e-01  8.395e-03 -1.787 0.0739\n\nLikelihood ratio test=23.36  on 7 df, p=0.001475\nn= 135, number of events= 97 \n\n\nVisual output of the model, using the latest version from Github of {mlr3viz}:\n\n\nCode\nautoplot(cox)\n\n\n\n\n\n\n\n\n\n\n\nPrediction types\nLet’s predict using the trained cox model on the test set (output is a PredictionSurv object):\n\n\nCode\np = cox$predict(task, row_ids = part$test)\np\n\n\n&lt;PredictionSurv&gt; for 33 observations:\n    row_ids time status       crank          lp     distr\n          1  455   TRUE -0.16022736 -0.16022736 &lt;list[1]&gt;\n          8  170   TRUE  0.07608537  0.07608537 &lt;list[1]&gt;\n         15  371   TRUE -0.46601841 -0.46601841 &lt;list[1]&gt;\n---                                                      \n        165  191  FALSE -0.30526841 -0.30526841 &lt;list[1]&gt;\n        166  105  FALSE  0.49632782  0.49632782 &lt;list[1]&gt;\n        168  177  FALSE -0.17234336 -0.17234336 &lt;list[1]&gt;\n\n\n\n\n\n\n\n\nPrediction types in mlr3proba\n\n\n\n\ncrank: Continuous risk ranking\nlp: Linear predictor calculated as \\hat\\beta * X_{test}\ndistr: Predicted survival distribution, either discrete or continuous\nresponse: Predicted survival time\n\n\n\nFor the cox model, crank = lp (the higher, the more risk):\n\n\nCode\np$lp\n\n\n           1            2            3            4            5            6 \n-0.160227364  0.076085366 -0.466018411  0.293380270  1.179147761  0.523244848 \n           7            8            9           10           11           12 \n 0.391564618 -0.029833700 -0.149489235 -0.262762070  0.076021387  0.279388934 \n          13           14           15           16           17           18 \n 0.889995280  0.859467193  1.030472975  0.277533930 -0.057165655  0.362416853 \n          19           20           21           22           23           24 \n-0.037670338 -0.295071061 -0.419840184  0.793214751  0.823500785  0.977222024 \n          25           26           27           28           29           30 \n-0.046252611  0.021227170 -0.093541236 -0.158438686  1.615114453  0.003701068 \n          31           32           33 \n-0.305268413  0.496327822 -0.172343361 \n\n\nSurvival prediction is a 2D matrix essentially, with dimensions: observations x time points:\n\n\nCode\np$data$distr[1:5, 1:5]\n\n\n          5        11        12        13        15\n1 0.9959775 0.9919519 0.9879072 0.9837970 0.9796477\n2 0.9949079 0.9898175 0.9847084 0.9795222 0.9742926\n3 0.9970357 0.9940659 0.9910789 0.9880402 0.9849691\n4 0.9936759 0.9873617 0.9810323 0.9746156 0.9681535\n5 0.9847342 0.9696296 0.9546262 0.9395560 0.9245212\n\n\nUsers should use the distr6 interface to access this prediction type, which allows us to retrieve survival probabilities (or hazards) for any time point of interest:\n\n\nCode\n# first 4 patients in the test set, specific time points:\np$distr[1:4]$survival(c(100, 500, 1200))\n\n\n          [,1]      [,2]      [,3]       [,4]\n100  0.9184997 0.8979186 0.9393041 0.87475634\n500  0.4589611 0.3729197 0.5634874 0.29352281\n1200 0.1684876 0.1048078 0.2693617 0.06062239\n\n\nVisualization of predicted survival curves for 3 test patients:\n\n\nCode\np2 = p$clone()$filter(row_ids = c(1,24,40))\nautoplot(p2, type = \"preds\")\n\n\n\n\n\n\n\n\n\n\n\nModel evaluation\n\n\n\n\n\n\nModel validation\n\n\n\nValidation of a survival model can be done by assessing:\n\nDiscrimination: the ability of the model to distinguish between low and high risk patients\nCalibration: the agreement between the observed and predicted survival probabilities\nOverall performance: the distance between the observed and predicted survival probabilities\n\n\n\nMany measures included in mlr3proba:\n\n\nCode\nmlr_measures$keys(pattern = \"surv\")\n\n\n [1] \"surv.brier\"         \"surv.calib_alpha\"   \"surv.calib_beta\"   \n [4] \"surv.chambless_auc\" \"surv.cindex\"        \"surv.dcalib\"       \n [7] \"surv.graf\"          \"surv.hung_auc\"      \"surv.intlogloss\"   \n[10] \"surv.logloss\"       \"surv.mae\"           \"surv.mse\"          \n[13] \"surv.nagelk_r2\"     \"surv.oquigley_r2\"   \"surv.rcll\"         \n[16] \"surv.rmse\"          \"surv.schmid\"        \"surv.song_auc\"     \n[19] \"surv.song_tnr\"      \"surv.song_tpr\"      \"surv.uno_auc\"      \n[22] \"surv.uno_tnr\"       \"surv.uno_tpr\"       \"surv.xu_r2\"        \n\n\nMost commonly used metrics are for assessing discrimination, such as Harrell’s C-index, Uno’s C-index and the (time-dependent) AUC:\n\n\nCode\nharrell_c = msr(\"surv.cindex\", id = \"surv.cindex.harrell\")\nuno_c = msr(\"surv.cindex\", weight_meth = \"G2\", id = \"surv.cindex.uno\")\nuno_auci = msr(\"surv.uno_auc\", integrated = TRUE) # across all times in the test set\nuno_auc = msr(\"surv.uno_auc\", integrated = FALSE, times = 10) # at a specific time-point of interest\n\nharrell_c\n\n\n&lt;MeasureSurvCindex:surv.cindex.harrell&gt;\n* Packages: mlr3, mlr3proba\n* Range: [0, 1]\n* Minimize: FALSE\n* Average: macro\n* Parameters: weight_meth=I, tiex=0.5, eps=0.001\n* Properties: -\n* Predict type: crank\n* Return type: Score\n\n\nCode\nuno_auc\n\n\n&lt;MeasureSurvUnoAUC:surv.uno_auc&gt;\n* Packages: mlr3, mlr3proba, survAUC\n* Range: [0, 1]\n* Minimize: FALSE\n* Average: macro\n* Parameters: integrated=FALSE, times=10\n* Properties: requires_task, requires_train_set\n* Predict type: lp\n* Return type: Score\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nNot all measures are applicable to all models - prediction type matters!\nMost discrimination metrics use the crank or lp prediction\n\n\n\n\n\nCode\np$score(harrell_c)\n\n\nsurv.cindex.harrell \n          0.6336898 \n\n\nCode\np$score(uno_c, task = task, train_set = part$train)\n\n\nsurv.cindex.uno \n      0.5907828 \n\n\nCalibration is traditionally performed graphically via calibration plots:\n\n\nCode\nautoplot(p, type = \"calib\", task = task, row_ids = part$test)\n\n\n\n\n\n\n\n\n\nBut there exists also calibration metrics, e.g. D-Calibration:\n\n\nCode\ndcal = msr(\"surv.dcalib\")\ndcal\n\n\n&lt;MeasureSurvDCalibration:surv.dcalib&gt;\n* Packages: mlr3, mlr3proba\n* Range: [0, Inf]\n* Minimize: TRUE\n* Average: macro\n* Parameters: B=10, chisq=FALSE, truncate=Inf\n* Properties: -\n* Predict type: distr\n* Return type: Score\n\n\nCode\np$score(dcal)\n\n\nsurv.dcalib \n   8.320423 \n\n\nOverall survival prediction performance can be assessed by scoring rules such as the Integrated Survival Brier Score (ISBS) and the Right-censored Log-Loss (RCLL) among others:\n\n\nCode\nrcll = msr(\"surv.rcll\")\nrcll\n\n\n&lt;MeasureSurvRCLL:surv.rcll&gt;\n* Packages: mlr3, mlr3proba, distr6\n* Range: [0, Inf]\n* Minimize: TRUE\n* Average: macro\n* Parameters: eps=1e-15, se=FALSE, ERV=FALSE, na.rm=TRUE\n* Properties: -\n* Predict type: distr\n* Return type: Score\n\n\nCode\np$score(rcll)\n\n\nsurv.rcll \n 23.46684 \n\n\n\n\nCode\nibrier = msr(\"surv.brier\", proper = TRUE)\nibrier\n\n\n&lt;MeasureSurvGraf:surv.graf&gt;\n* Packages: mlr3, mlr3proba\n* Range: [0, Inf]\n* Minimize: TRUE\n* Average: macro\n* Parameters: integrated=TRUE, method=2, se=FALSE, proper=TRUE,\n  eps=0.001, ERV=FALSE\n* Properties: -\n* Predict type: distr\n* Return type: Score\n\n\nCode\np$score(ibrier, task = task, train_set = part$train)\n\n\nsurv.graf \n0.1591112"
  },
  {
    "objectID": "index.html#using-ml-survival-models-on-high-dimensional-data",
    "href": "index.html#using-ml-survival-models-on-high-dimensional-data",
    "title": "Intro to Machine Learning for Survival Analysis with mlr3",
    "section": "Using ML survival models on high-dimensional data",
    "text": "Using ML survival models on high-dimensional data\nSo far we have used the Cox regression model, but there are many more machine learning methods available via mlr3extralearners! We will take a look at the following:\n\nCox elastic net via glmnet\n\nWe will use lrn(\"surv.cv_glmnet\"), wich internally tunes for lambda using cross-validation\n\nLikelihood-based boosting via CoxBoost\n\nWe will use lrn(\"surv.cv_coxboost\", penalty = \"optimCoxBoostPenalty\"), which also uses internal cross-validation to tune its parameters\n\nRandom Forests via ranger\nOblique Random Forests via aorsf\n\nThese learners then cover the range from penalized regression to tree ensembles and boosting.\nLet’s take these learners for a spin on a subset of TCGA breast cancer data with gene expression and clinical features. We first need to create a TaskSurv object from the data, which we can do by reading in the data and then using as_task_surv(). We also add the status column to the stratum, which is necessary for the resampling to ensure a similar proportion of events in the resampling folds than the complete dataset.\n\n\nCode\ntcga = readRDS(\"data/tcga.rds\")\n\ntask_tcga &lt;- mlr3proba::as_task_surv(\n  x = tcga, \n  time = \"time\", event = \"status\", id = \"BRCA-TCGA\"\n)\n\n# Set stratum for resampling\ntask$set_col_roles(\"status\", add_to = \"stratum\")\n\n\nWe can instantiate our learners as we’ve seen before — we’re sticking to mostly vanilla settings for now.\nWe can let glmnet determine the optimal value for lambda with it’s internal cross-validation method Similarly, CoxBoost could tune itself, but we’ll stick with a simple version to save some time on compute! For the forests, we use 100 trees each for speed and otherwise accept the defaults.\n\n\nCode\nlrn_glmnet = lrn(\"surv.cv_glmnet\", alpha = 0.5)\nlrn_coxboost = lrn(\"surv.coxboost\", penalty = 100)\nlrn_ranger = lrn(\"surv.ranger\", num.trees = 100)\nlrn_aorsf = lrn(\"surv.aorsf\", n_tree = 100)\n\n\nWe can now use resample() to evaluate the performance of each of these learners on the task. To to this, we decide on two measures: Harrell’s C and the integrated brier score, and we also instantiate a resampling to use for comparison, such that we ensure all learners see the same data.\n\n\nCode\nmeasures = list(msr(\"surv.cindex\", id = \"cindex\"), msr(\"surv.brier\", id = \"ibs\"))\n\nresampling = rsmp(\"cv\", folds = 3)\nresampling$instantiate(task_tcga)\n\nrr_glmnet = resample(\n  task = task_tcga,\n  learner = lrn_glmnet,\n  resampling = resampling\n)\n\n\nINFO  [17:59:23.620] [mlr3] Applying learner 'surv.cv_glmnet' on task 'BRCA-TCGA' (iter 1/3)\nINFO  [17:59:24.859] [mlr3] Applying learner 'surv.cv_glmnet' on task 'BRCA-TCGA' (iter 2/3)\nINFO  [17:59:25.679] [mlr3] Applying learner 'surv.cv_glmnet' on task 'BRCA-TCGA' (iter 3/3)\n\n\nCode\nrr_glmnet$score(measures)\n\n\n     task_id     learner_id resampling_id iteration cindex       ibs\n      &lt;char&gt;         &lt;char&gt;        &lt;char&gt;     &lt;int&gt;  &lt;num&gt;     &lt;num&gt;\n1: BRCA-TCGA surv.cv_glmnet            cv         1    0.5 0.1756614\n2: BRCA-TCGA surv.cv_glmnet            cv         2    0.5 0.1324771\n3: BRCA-TCGA surv.cv_glmnet            cv         3    0.5 0.2651273\nHidden columns: task, learner, resampling, prediction\n\n\nWell, looks like glmnet out of the box does not do well on this dataset, judging by the C-index of 0.5. This is what the null model achieves, after all!\nFeel free to play with the parameters of glmnet a bit more — for example, does changing alpha help?\nWe can repeat the same procedure for the other learners:\n\n\nCode\nrr_coxboost = resample(\n  task = task_tcga,\n  learner = lrn_coxboost,\n  resampling = resampling\n)\n\n\nINFO  [17:59:26.852] [mlr3] Applying learner 'surv.coxboost' on task 'BRCA-TCGA' (iter 1/3)\nINFO  [17:59:27.528] [mlr3] Applying learner 'surv.coxboost' on task 'BRCA-TCGA' (iter 2/3)\nINFO  [17:59:28.009] [mlr3] Applying learner 'surv.coxboost' on task 'BRCA-TCGA' (iter 3/3)\n\n\nCode\nrr_ranger = resample(\n  task = task_tcga,\n  learner = lrn_ranger,\n  resampling = resampling\n)\n\n\nINFO  [17:59:28.499] [mlr3] Applying learner 'surv.ranger' on task 'BRCA-TCGA' (iter 1/3)\nINFO  [17:59:30.820] [mlr3] Applying learner 'surv.ranger' on task 'BRCA-TCGA' (iter 2/3)\nINFO  [17:59:33.572] [mlr3] Applying learner 'surv.ranger' on task 'BRCA-TCGA' (iter 3/3)\n\n\nCode\nrr_aorsf = resample(\n  task = task_tcga,\n  learner = lrn_aorsf,\n  resampling = resampling\n)\n\n\nINFO  [17:59:36.629] [mlr3] Applying learner 'surv.aorsf' on task 'BRCA-TCGA' (iter 1/3)\nINFO  [17:59:36.731] [mlr3] Applying learner 'surv.aorsf' on task 'BRCA-TCGA' (iter 2/3)\nINFO  [17:59:36.772] [mlr3] Applying learner 'surv.aorsf' on task 'BRCA-TCGA' (iter 3/3)\n\n\nCode\nrr_coxboost$score(measures)\n\n\n     task_id    learner_id resampling_id iteration    cindex       ibs\n      &lt;char&gt;        &lt;char&gt;        &lt;char&gt;     &lt;int&gt;     &lt;num&gt;     &lt;num&gt;\n1: BRCA-TCGA surv.coxboost            cv         1 0.6715095 0.2181875\n2: BRCA-TCGA surv.coxboost            cv         2 0.5843462 0.1397133\n3: BRCA-TCGA surv.coxboost            cv         3 0.5756824 0.2642506\nHidden columns: task, learner, resampling, prediction\n\n\nCode\nrr_ranger$score(measures)\n\n\n     task_id  learner_id resampling_id iteration    cindex       ibs\n      &lt;char&gt;      &lt;char&gt;        &lt;char&gt;     &lt;int&gt;     &lt;num&gt;     &lt;num&gt;\n1: BRCA-TCGA surv.ranger            cv         1 0.6609037 0.2281387\n2: BRCA-TCGA surv.ranger            cv         2 0.6891946 0.1394615\n3: BRCA-TCGA surv.ranger            cv         3 0.5599183 0.2884375\nHidden columns: task, learner, resampling, prediction\n\n\nCode\nrr_aorsf$score(measures)\n\n\n     task_id learner_id resampling_id iteration    cindex       ibs\n      &lt;char&gt;     &lt;char&gt;        &lt;char&gt;     &lt;int&gt;     &lt;num&gt;     &lt;num&gt;\n1: BRCA-TCGA surv.aorsf            cv         1 0.6649716 0.2139267\n2: BRCA-TCGA surv.aorsf            cv         2 0.6576733 0.1297630\n3: BRCA-TCGA surv.aorsf            cv         3 0.5644432 0.2737022\nHidden columns: task, learner, resampling, prediction\n\n\nNow we have a comparison of the performance of the different learners on the task. We can again aggregate these results to get a summary of the performance of each learner across all resamplings:\n\n\nCode\nrr_glmnet$aggregate(measures)\n\n\n   cindex       ibs \n0.5000000 0.1910886 \n\n\nCode\nrr_coxboost$aggregate(measures)\n\n\n   cindex       ibs \n0.6105127 0.2073838 \n\n\nCode\nrr_ranger$aggregate(measures)\n\n\n   cindex       ibs \n0.6366722 0.2186793 \n\n\nCode\nrr_aorsf$aggregate(measures)\n\n\n   cindex       ibs \n0.6290294 0.2057973 \n\n\nNow, for a quick example on a single dataset this approach is goon enough, but a bit cumbersome. What we essentially did here was a naive benchmarking of the learners on the task — something mlr3 has dedicated tools for!\nWe can perform the same procedure by first defining a benchmark design of one or more tasks and at least two learners like so:\n\n\nCode\ndesign = benchmark_grid(\n  tasks = task_tcga,\n  learners = list(lrn_glmnet, lrn_ranger, lrn_aorsf, lrn_coxboost),\n  resamplings = resampling\n)\n\ndesign\n\n\n        task        learner resampling\n      &lt;char&gt;         &lt;char&gt;     &lt;char&gt;\n1: BRCA-TCGA surv.cv_glmnet         cv\n2: BRCA-TCGA    surv.ranger         cv\n3: BRCA-TCGA     surv.aorsf         cv\n4: BRCA-TCGA  surv.coxboost         cv\n\n\nTo perform the benchmark, we use the aptly named benchmark() function, which will perform the necessary resampling iterations and store the results for us:\n\n\nINFO  [17:59:37.838] [mlr3] Running benchmark with 12 resampling iterations\nINFO  [17:59:37.841] [mlr3] Applying learner 'surv.cv_glmnet' on task 'BRCA-TCGA' (iter 1/3)\nINFO  [17:59:38.848] [mlr3] Applying learner 'surv.cv_glmnet' on task 'BRCA-TCGA' (iter 2/3)\nINFO  [17:59:39.686] [mlr3] Applying learner 'surv.cv_glmnet' on task 'BRCA-TCGA' (iter 3/3)\nINFO  [17:59:40.740] [mlr3] Applying learner 'surv.ranger' on task 'BRCA-TCGA' (iter 1/3)\nINFO  [17:59:43.115] [mlr3] Applying learner 'surv.ranger' on task 'BRCA-TCGA' (iter 2/3)\nINFO  [17:59:45.405] [mlr3] Applying learner 'surv.ranger' on task 'BRCA-TCGA' (iter 3/3)\nINFO  [17:59:47.909] [mlr3] Applying learner 'surv.aorsf' on task 'BRCA-TCGA' (iter 1/3)\nINFO  [17:59:47.946] [mlr3] Applying learner 'surv.aorsf' on task 'BRCA-TCGA' (iter 2/3)\nINFO  [17:59:47.983] [mlr3] Applying learner 'surv.aorsf' on task 'BRCA-TCGA' (iter 3/3)\nINFO  [17:59:48.026] [mlr3] Applying learner 'surv.coxboost' on task 'BRCA-TCGA' (iter 1/3)\nINFO  [17:59:48.634] [mlr3] Applying learner 'surv.coxboost' on task 'BRCA-TCGA' (iter 2/3)\nINFO  [17:59:49.118] [mlr3] Applying learner 'surv.coxboost' on task 'BRCA-TCGA' (iter 3/3)\nINFO  [17:59:49.561] [mlr3] Finished benchmark\n\n\n&lt;BenchmarkResult&gt; of 12 rows with 4 resampling runs\n nr   task_id     learner_id resampling_id iters warnings errors\n  1 BRCA-TCGA surv.cv_glmnet            cv     3        0      0\n  2 BRCA-TCGA    surv.ranger            cv     3        0      0\n  3 BRCA-TCGA     surv.aorsf            cv     3        0      0\n  4 BRCA-TCGA  surv.coxboost            cv     3        0      0\n\n\nWhen we $score() or $aggregate() the benchmark result, we should get the same exact scores as before because we used the instantiated resampling from earlier, meaning each elarner again saw the same data:\n\n\nCode\nbmr$score(measures)\n\n\n       nr   task_id     learner_id resampling_id iteration    cindex       ibs\n    &lt;int&gt;    &lt;char&gt;         &lt;char&gt;        &lt;char&gt;     &lt;int&gt;     &lt;num&gt;     &lt;num&gt;\n 1:     1 BRCA-TCGA surv.cv_glmnet            cv         1 0.5000000 0.1756614\n 2:     1 BRCA-TCGA surv.cv_glmnet            cv         2 0.5000000 0.1324771\n 3:     1 BRCA-TCGA surv.cv_glmnet            cv         3 0.5000000 0.2651273\n 4:     2 BRCA-TCGA    surv.ranger            cv         1 0.6834229 0.2226761\n 5:     2 BRCA-TCGA    surv.ranger            cv         2 0.6858555 0.1402617\n 6:     2 BRCA-TCGA    surv.ranger            cv         3 0.5527660 0.2897589\n 7:     3 BRCA-TCGA     surv.aorsf            cv         1 0.6579980 0.2097476\n 8:     3 BRCA-TCGA     surv.aorsf            cv         2 0.6407106 0.1320248\n 9:     3 BRCA-TCGA     surv.aorsf            cv         3 0.5804992 0.2769839\n10:     4 BRCA-TCGA  surv.coxboost            cv         1 0.6715095 0.2181875\n11:     4 BRCA-TCGA  surv.coxboost            cv         2 0.5843462 0.1397133\n12:     4 BRCA-TCGA  surv.coxboost            cv         3 0.5756824 0.2642506\nHidden columns: uhash, task, learner, resampling, prediction\n\n\nCode\nbmr$aggregate(measures)\n\n\n      nr   task_id     learner_id resampling_id iters    cindex       ibs\n   &lt;int&gt;    &lt;char&gt;         &lt;char&gt;        &lt;char&gt; &lt;int&gt;     &lt;num&gt;     &lt;num&gt;\n1:     1 BRCA-TCGA surv.cv_glmnet            cv     3 0.5000000 0.1910886\n2:     2 BRCA-TCGA    surv.ranger            cv     3 0.6406815 0.2175656\n3:     3 BRCA-TCGA     surv.aorsf            cv     3 0.6264026 0.2062521\n4:     4 BRCA-TCGA  surv.coxboost            cv     3 0.6105127 0.2073838\nHidden columns: resample_result\n\n\nWe can also visualize the results — see ?autoplot.BenchmarkResult for more options:\n\n\nCode\nautoplot(bmr, type = \"boxplot\", measure = msr(\"surv.brier\"))\n\n\n\n\n\n\n\n\n\nFrom our quick tests, which learner now seems to have done the best? Given that we used these learners more or less off the shelf without tuning, we should not put too much weight on these results, but it’s a good starting point for further exploration!\nTuning is a complex topic and you can learn more about it in the mlr3book chapter, but unfortunately we don’t have time to cover it here!"
  },
  {
    "objectID": "index.html#benchmarking-with-multiple-datasets",
    "href": "index.html#benchmarking-with-multiple-datasets",
    "title": "Intro to Machine Learning for Survival Analysis with mlr3",
    "section": "Benchmarking with multiple datasets",
    "text": "Benchmarking with multiple datasets\n\n\n\n\n\n\nTeaching Aims\n\n\n\n\nPerform a small-scale benchmark\nAggregate and visualize the results\nPerform a statistical analysis of the results\n\n\n\nA proper benchmark can take a lot of time and planning, but it can pay off to get a good overview of the performance of different learners on different tasks relevant to your field!\nIn this example, we’ll take a number of small datasets provided by mlr3proba and benchmark the learners we used before on them. These tasks are small enough to hopefully not spend too much time waiting for computations to finish, but we hope you get enough of an idea to feel confident to perform your own experiments!\nThe procedure is as follows:\n\nGather tasks as a list.\nGather our learners. Normally this would include deciding on tuning spaces!\nDefine a resampling strategy.\nDecide on measures to use.\n\nFor step 1, we’ll select some survival tasks from mlr_tasks for this benchmark:\n\n\nCode\ntasks = list(\n  tsk(\"actg\"),\n  tsk(\"gbcs\"),\n  tsk(\"grace\"),\n  tsk(\"lung\"),\n  tsk(\"mgus\")\n)\n\ntasks\n\n\n[[1]]\n&lt;TaskSurv:actg&gt; (1151 x 13): ACTG 320\n* Target: time, status\n* Properties: -\n* Features (11):\n  - dbl (4): age, cd4, priorzdv, sexF\n  - fct (4): ivdrug, karnof, raceth, txgrp\n  - int (3): hemophil, strat2, tx\n\n[[2]]\n&lt;TaskSurv:gbcs&gt; (686 x 10): German Breast Cancer\n* Target: time, status\n* Properties: -\n* Features (8):\n  - dbl (4): age, estrg_recp, prog_recp, size\n  - int (4): grade, hormone, menopause, nodes\n\n[[3]]\n&lt;TaskSurv:grace&gt; (1000 x 8): GRACE 1000\n* Target: time, status\n* Properties: -\n* Features (6):\n  - dbl (4): age, los, revascdays, sysbp\n  - int (2): revasc, stchange\n\n[[4]]\n&lt;TaskSurv:lung&gt; (168 x 9): Lung Cancer\n* Target: time, status\n* Properties: -\n* Features (7):\n  - int (6): age, meal.cal, pat.karno, ph.ecog, ph.karno, wt.loss\n  - fct (1): sex\n\n[[5]]\n&lt;TaskSurv:mgus&gt; (176 x 9): MGUS\n* Target: time, status\n* Properties: -\n* Features (7):\n  - dbl (6): age, alb, creat, dxyr, hgb, mspike\n  - fct (1): sex\n\n\nMany have categorical features (fct), which can be a bit tricky to handle for some learners, so we will take a shortcut and add a feature encoding PipeOp to the learners that need it. Pipelines and preprocessing are very useful, and the mlr3book again has you covered!\nWe use the po(\"encode\") pipe operator to encode the factors as dummy-encoded variables (method = \"treatment\") for the Cox model, and use the default (one-hot encoding) for the others. The %&gt;&gt;% operator is used to chain PipeOps and learners together, and we wrap the pipeline in as_learner such that we can treat it as a learner just like the others.\n\n\nCode\npreproc = po(\"encode\", method = \"treatment\")\n\nlearners = list(\n  cox = as_learner(preproc %&gt;&gt;% lrn(\"surv.coxph\",id = \"cph\")),\n  glmnet = as_learner(preproc %&gt;&gt;% lrn(\"surv.cv_glmnet\", alpha = 0.5)),\n  ranger = lrn(\"surv.ranger\", num.trees = 100),\n  aorsf = lrn(\"surv.aorsf\", n_tree = 100),\n  coxboost = as_learner(preproc %&gt;&gt;% lrn(\"surv.coxboost\", penalty = 100))\n)\n\n\nA small convenience thing we can do here is to set IDs for the learners, which will make the output of further steps more readable:\n\n\n$cox\n[1] \"cox\"\n\n$glmnet\n[1] \"glmnet\"\n\n$ranger\n[1] \"ranger\"\n\n$aorsf\n[1] \"aorsf\"\n\n$coxboost\n[1] \"coxboost\"\n\n\nMoving on to the benchmark, we create a design grid as before, only now we have multiple tasks. Luckily, benchmark_grid() can handle this for us by instantiating the resampling for each task, so we don’t have to worry about this here!\n\n\nCode\ndesign = benchmark_grid(\n  tasks = tasks,\n  learners = learners,\n  resamplings = rsmp(\"cv\", folds = 3)\n)\n\ndesign\n\n\n      task  learner resampling\n    &lt;char&gt;   &lt;char&gt;     &lt;char&gt;\n 1:   actg      cox         cv\n 2:   actg   glmnet         cv\n 3:   actg   ranger         cv\n 4:   actg    aorsf         cv\n 5:   actg coxboost         cv\n 6:   gbcs      cox         cv\n 7:   gbcs   glmnet         cv\n 8:   gbcs   ranger         cv\n 9:   gbcs    aorsf         cv\n10:   gbcs coxboost         cv\n11:  grace      cox         cv\n12:  grace   glmnet         cv\n13:  grace   ranger         cv\n14:  grace    aorsf         cv\n15:  grace coxboost         cv\n16:   lung      cox         cv\n17:   lung   glmnet         cv\n18:   lung   ranger         cv\n19:   lung    aorsf         cv\n20:   lung coxboost         cv\n21:   mgus      cox         cv\n22:   mgus   glmnet         cv\n23:   mgus   ranger         cv\n24:   mgus    aorsf         cv\n25:   mgus coxboost         cv\n      task  learner resampling\n\n\n\n\nINFO  [17:59:51.782] [mlr3] Running benchmark with 75 resampling iterations\nINFO  [17:59:51.785] [mlr3] Applying learner 'cox' on task 'actg' (iter 1/3)\nINFO  [17:59:51.847] [mlr3] Applying learner 'cox' on task 'actg' (iter 2/3)\nINFO  [17:59:51.915] [mlr3] Applying learner 'cox' on task 'actg' (iter 3/3)\nINFO  [17:59:51.974] [mlr3] Applying learner 'glmnet' on task 'actg' (iter 1/3)\nINFO  [17:59:52.280] [mlr3] Applying learner 'glmnet' on task 'actg' (iter 2/3)\nINFO  [17:59:52.626] [mlr3] Applying learner 'glmnet' on task 'actg' (iter 3/3)\nINFO  [17:59:52.947] [mlr3] Applying learner 'ranger' on task 'actg' (iter 1/3)\nINFO  [17:59:53.111] [mlr3] Applying learner 'ranger' on task 'actg' (iter 2/3)\nINFO  [17:59:53.314] [mlr3] Applying learner 'ranger' on task 'actg' (iter 3/3)\nINFO  [17:59:53.484] [mlr3] Applying learner 'aorsf' on task 'actg' (iter 1/3)\nINFO  [17:59:53.514] [mlr3] Applying learner 'aorsf' on task 'actg' (iter 2/3)\nINFO  [17:59:53.542] [mlr3] Applying learner 'aorsf' on task 'actg' (iter 3/3)\nINFO  [17:59:53.568] [mlr3] Applying learner 'coxboost' on task 'actg' (iter 1/3)\nINFO  [17:59:54.012] [mlr3] Applying learner 'coxboost' on task 'actg' (iter 2/3)\nINFO  [17:59:54.291] [mlr3] Applying learner 'coxboost' on task 'actg' (iter 3/3)\nINFO  [17:59:54.549] [mlr3] Applying learner 'cox' on task 'gbcs' (iter 1/3)\nINFO  [17:59:54.589] [mlr3] Applying learner 'cox' on task 'gbcs' (iter 2/3)\nINFO  [17:59:54.623] [mlr3] Applying learner 'cox' on task 'gbcs' (iter 3/3)\nINFO  [17:59:54.656] [mlr3] Applying learner 'glmnet' on task 'gbcs' (iter 1/3)\nINFO  [17:59:54.756] [mlr3] Applying learner 'glmnet' on task 'gbcs' (iter 2/3)\nINFO  [17:59:54.850] [mlr3] Applying learner 'glmnet' on task 'gbcs' (iter 3/3)\nINFO  [17:59:54.953] [mlr3] Applying learner 'ranger' on task 'gbcs' (iter 1/3)\nINFO  [17:59:55.112] [mlr3] Applying learner 'ranger' on task 'gbcs' (iter 2/3)\nINFO  [17:59:55.273] [mlr3] Applying learner 'ranger' on task 'gbcs' (iter 3/3)\nINFO  [17:59:55.449] [mlr3] Applying learner 'aorsf' on task 'gbcs' (iter 1/3)\nINFO  [17:59:55.472] [mlr3] Applying learner 'aorsf' on task 'gbcs' (iter 2/3)\nINFO  [17:59:55.502] [mlr3] Applying learner 'aorsf' on task 'gbcs' (iter 3/3)\nINFO  [17:59:55.525] [mlr3] Applying learner 'coxboost' on task 'gbcs' (iter 1/3)\nINFO  [17:59:55.734] [mlr3] Applying learner 'coxboost' on task 'gbcs' (iter 2/3)\nINFO  [17:59:55.930] [mlr3] Applying learner 'coxboost' on task 'gbcs' (iter 3/3)\nINFO  [17:59:56.311] [mlr3] Applying learner 'cox' on task 'grace' (iter 1/3)\nINFO  [17:59:56.343] [mlr3] Applying learner 'cox' on task 'grace' (iter 2/3)\nINFO  [17:59:56.374] [mlr3] Applying learner 'cox' on task 'grace' (iter 3/3)\nINFO  [17:59:56.416] [mlr3] Applying learner 'glmnet' on task 'grace' (iter 1/3)\nINFO  [17:59:56.554] [mlr3] Applying learner 'glmnet' on task 'grace' (iter 2/3)\nINFO  [17:59:56.697] [mlr3] Applying learner 'glmnet' on task 'grace' (iter 3/3)\nINFO  [17:59:56.834] [mlr3] Applying learner 'ranger' on task 'grace' (iter 1/3)\nINFO  [17:59:57.061] [mlr3] Applying learner 'ranger' on task 'grace' (iter 2/3)\nINFO  [17:59:57.254] [mlr3] Applying learner 'ranger' on task 'grace' (iter 3/3)\nINFO  [17:59:57.458] [mlr3] Applying learner 'aorsf' on task 'grace' (iter 1/3)\nINFO  [17:59:57.501] [mlr3] Applying learner 'aorsf' on task 'grace' (iter 2/3)\nINFO  [17:59:57.532] [mlr3] Applying learner 'aorsf' on task 'grace' (iter 3/3)\nINFO  [17:59:57.564] [mlr3] Applying learner 'coxboost' on task 'grace' (iter 1/3)\nINFO  [17:59:57.941] [mlr3] Applying learner 'coxboost' on task 'grace' (iter 2/3)\nINFO  [17:59:58.268] [mlr3] Applying learner 'coxboost' on task 'grace' (iter 3/3)\nINFO  [17:59:58.611] [mlr3] Applying learner 'cox' on task 'lung' (iter 1/3)\nINFO  [17:59:58.653] [mlr3] Applying learner 'cox' on task 'lung' (iter 2/3)\nINFO  [17:59:58.695] [mlr3] Applying learner 'cox' on task 'lung' (iter 3/3)\nINFO  [17:59:58.930] [mlr3] Applying learner 'glmnet' on task 'lung' (iter 1/3)\nINFO  [17:59:58.998] [mlr3] Applying learner 'glmnet' on task 'lung' (iter 2/3)\nINFO  [17:59:59.066] [mlr3] Applying learner 'glmnet' on task 'lung' (iter 3/3)\nINFO  [17:59:59.147] [mlr3] Applying learner 'ranger' on task 'lung' (iter 1/3)\nINFO  [17:59:59.182] [mlr3] Applying learner 'ranger' on task 'lung' (iter 2/3)\nINFO  [17:59:59.218] [mlr3] Applying learner 'ranger' on task 'lung' (iter 3/3)\nINFO  [17:59:59.256] [mlr3] Applying learner 'aorsf' on task 'lung' (iter 1/3)\nINFO  [17:59:59.273] [mlr3] Applying learner 'aorsf' on task 'lung' (iter 2/3)\nINFO  [17:59:59.291] [mlr3] Applying learner 'aorsf' on task 'lung' (iter 3/3)\nINFO  [17:59:59.308] [mlr3] Applying learner 'coxboost' on task 'lung' (iter 1/3)\nINFO  [17:59:59.399] [mlr3] Applying learner 'coxboost' on task 'lung' (iter 2/3)\nINFO  [17:59:59.518] [mlr3] Applying learner 'coxboost' on task 'lung' (iter 3/3)\nINFO  [17:59:59.628] [mlr3] Applying learner 'cox' on task 'mgus' (iter 1/3)\nINFO  [17:59:59.670] [mlr3] Applying learner 'cox' on task 'mgus' (iter 2/3)\nINFO  [17:59:59.711] [mlr3] Applying learner 'cox' on task 'mgus' (iter 3/3)\nINFO  [17:59:59.754] [mlr3] Applying learner 'glmnet' on task 'mgus' (iter 1/3)\nINFO  [17:59:59.831] [mlr3] Applying learner 'glmnet' on task 'mgus' (iter 2/3)\nINFO  [17:59:59.898] [mlr3] Applying learner 'glmnet' on task 'mgus' (iter 3/3)\nINFO  [17:59:59.967] [mlr3] Applying learner 'ranger' on task 'mgus' (iter 1/3)\nINFO  [18:00:00.013] [mlr3] Applying learner 'ranger' on task 'mgus' (iter 2/3)\nINFO  [18:00:00.064] [mlr3] Applying learner 'ranger' on task 'mgus' (iter 3/3)\nINFO  [18:00:00.110] [mlr3] Applying learner 'aorsf' on task 'mgus' (iter 1/3)\nINFO  [18:00:00.129] [mlr3] Applying learner 'aorsf' on task 'mgus' (iter 2/3)\nINFO  [18:00:00.148] [mlr3] Applying learner 'aorsf' on task 'mgus' (iter 3/3)\nINFO  [18:00:00.166] [mlr3] Applying learner 'coxboost' on task 'mgus' (iter 1/3)\nINFO  [18:00:00.309] [mlr3] Applying learner 'coxboost' on task 'mgus' (iter 2/3)\nINFO  [18:00:00.449] [mlr3] Applying learner 'coxboost' on task 'mgus' (iter 3/3)\n\n\nWarning in coxph.fit(X, Y, istrat, offset, init, control, weights = weights, : Loglik converged before variable  7,17 ; coefficient may be infinite. \nThis happened PipeOp cph's $train()\n\n\nWarning in coxph.fit(X, Y, istrat, offset, init, control, weights = weights, : Loglik converged before variable  7,16,17,18 ; coefficient may be infinite. \nThis happened PipeOp cph's $train()\n\n\nWarning in coxph.fit(X, Y, istrat, offset, init, control, weights = weights, : Loglik converged before variable  8,18 ; coefficient may be infinite. \nThis happened PipeOp cph's $train()\n\n\nINFO  [18:00:00.596] [mlr3] Finished benchmark\n\n\nWe pick the IBS again and aggregate the results:\n\n\nCode\nmeasure = msr(\"surv.brier\", id = \"ibs\")\n\nbmr$aggregate(measure)\n\n\n       nr task_id learner_id resampling_id iters        ibs\n    &lt;int&gt;  &lt;char&gt;     &lt;char&gt;        &lt;char&gt; &lt;int&gt;      &lt;num&gt;\n 1:     1    actg        cox            cv     3 0.05978873\n 2:     2    actg     glmnet            cv     3 0.06041163\n 3:     3    actg     ranger            cv     3 0.06088070\n 4:     4    actg      aorsf            cv     3 0.05837878\n 5:     5    actg   coxboost            cv     3 0.05906567\n 6:     6    gbcs        cox            cv     3 0.12084849\n 7:     7    gbcs     glmnet            cv     3 0.13339647\n 8:     8    gbcs     ranger            cv     3 0.12825468\n 9:     9    gbcs      aorsf            cv     3 0.11956394\n10:    10    gbcs   coxboost            cv     3 0.12084801\n11:    11   grace        cox            cv     3 0.09749663\n12:    12   grace     glmnet            cv     3 0.10381720\n13:    13   grace     ranger            cv     3 0.10787449\n14:    14   grace      aorsf            cv     3 0.09184122\n15:    15   grace   coxboost            cv     3 0.09749658\n [ reached getOption(\"max.print\") -- omitted 11 rows ]\nHidden columns: resample_result\n\n\n\nStatistical analysis\nRather than just computing average scores, we can leverage mlr3benchmark for additional analysis steps, including a statistical analysis of the results. The starting point is to convert the benchmark result (bmr) to an aggregated benchmark result (bma), which is a more convenient format for further analysis:\n\n\nCode\nlibrary(mlr3benchmark)\nbma = as_benchmark_aggr(bmr, meas = measure)\nbma\n\n\n&lt;BenchmarkAggr&gt; of 25 rows with 5 tasks, 5 learners and 1 measure\n    task_id learner_id        ibs\n     &lt;fctr&gt;     &lt;fctr&gt;      &lt;num&gt;\n 1:    actg        cox 0.05978873\n 2:    actg     glmnet 0.06041163\n 3:    actg     ranger 0.06088070\n 4:    actg      aorsf 0.05837878\n 5:    actg   coxboost 0.05906567\n 6:    gbcs        cox 0.12084849\n 7:    gbcs     glmnet 0.13339647\n 8:    gbcs     ranger 0.12825468\n 9:    gbcs      aorsf 0.11956394\n10:    gbcs   coxboost 0.12084801\n11:   grace        cox 0.09749663\n12:   grace     glmnet 0.10381720\n13:   grace     ranger 0.10787449\n14:   grace      aorsf 0.09184122\n15:   grace   coxboost 0.09749658\n16:    lung        cox 0.15922764\n17:    lung     glmnet 0.14848136\n18:    lung     ranger 0.17343481\n19:    lung      aorsf 0.15450793\n20:    lung   coxboost 0.15905475\n21:    mgus        cox 0.12491045\n22:    mgus     glmnet 0.13442554\n23:    mgus     ranger 0.14613226\n24:    mgus      aorsf 0.13580314\n25:    mgus   coxboost 0.12491102\n    task_id learner_id        ibs\n\n\nThis brings with it a few more autoplot methods, see ?autoplot.BenchmarkAggr.\n\n\nCode\nautoplot(bma, type = \"box\", meas = \"ibs\")\n\n\n\n\n\n\n\n\n\nFor the statistical analysis, we can use a simple rank-based analysis with a global Friedman test to see if there are significant differences between the learners:\n\n\nCode\nbma$friedman_test()\n\n\n\n    Friedman rank sum test\n\ndata:  ibs and learner_id and task_id\nFriedman chi-squared = 11.04, df = 4, p-value = 0.02612\n\n\nThe corresponding post-hoc test for all pairwise comparison can be performed as follows:\n\n\nCode\nbma$friedman_posthoc()\n\n\n\n    Pairwise comparisons using Nemenyi-Wilcoxon-Wilcox all-pairs test for a two-way balanced complete block design\n\n\ndata: ibs and learner_id and task_id\n\n\n         cox   glmnet ranger aorsf\nglmnet   0.975 -      -      -    \nranger   0.266 0.628  -      -    \naorsf    0.855 0.497  0.023  -    \ncoxboost 0.975 0.751  0.070  0.995\n\n\n\nP value adjustment method: single-step\n\n\nWhich is also available graphically:\n\n\nCode\nautoplot(bma, type = \"fn\", meas = \"ibs\")"
  },
  {
    "objectID": "index.html#conclusion",
    "href": "index.html#conclusion",
    "title": "Intro to Machine Learning for Survival Analysis with mlr3",
    "section": "Conclusion",
    "text": "Conclusion\nWe have conducted a tiny benchmark experiment on a few survival tasks using a few learners — a good starting point for further exploration! Advanced topics we did not cover in more detail include tuning and more advanced pipelines, but we hope you got a good overview of the capabilities of mlr3proba and mlr3 in general."
  }
]