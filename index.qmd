---
title: "Intro to Machine Learning for Survival Analysis with mlr3"
author: "[John Zobolas](https://github.com/bblodfon), [Lukas Burk](https://lukasburk.de/)"
date: last-modified
description: "Tutorial for the useR! 2024 conference in Salzburg, Austria (8-11 July)"
format:
  html:
    date: last-modified
    code-block-bg: true
    code-copy: true
    code-fold: show
    code-overflow: wrap
    code-block-border-left: true
    toc: true
    toc-location: left
    html-math-method: katex
    page-layout: full
execute:
  freeze: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = FALSE)
```

## `mlr3`: Basics {-}

:::{.callout-tip title="Teaching Aims"}
- Understand how `{mlr3}` is structured
- Access `learner`s and (built-in) `task`s
:::

To get started, we load `{mlr3verse}`, which will load various packages from the `{mlr3}` ecosystem:

```{r, message=FALSE, warning=FALSE}
library(mlr3verse)
```

`{mlr3}` ships with wrappers for many commonly used machine learning algorithms ("learners").  
We can access the list of available learners using the `mlr_learners` dictionary:
```{r}
sample(mlr_learners$keys(), 10)
```

One example:
```{r}
lrn("classif.ranger")
```

:::{.callout-note}
Use `lrn("classif.ranger")$help()` to view the help page, with links to documentation for parameters and other information about the wrapped learner.
:::

Built-in tasks can be accessed using the `mlr_tasks` dictionary:
```{r}
head(as.data.table(mlr_tasks)[, list(key, label, task_type, nrow, ncol, properties)])
```

One example:

```{r}
tsk("penguins_simple")
```

:::{.callout-note}
Tasks encapsulate a data source (typically a `data.table`) and additional information regarding which variables are considered features and target.
Tasks can also specify additional properties such as stratification, which we will see later.
:::

## Example: Train-Predict-Evaluate {-}

:::{.callout-tip title="Teaching Aims"}
- Perform a simple train-predict-evaluate step
- Use built-in classification `task` and `learner`
:::

The below code snippet trains a random forest model on the `penguins_simple` task (a simplified version of the `palmerpenguins` dataset, but without missing values) and evaluates the model's performance using the classification error metric:

```{r}
task = tsk("penguins_simple")
learner = lrn("classif.ranger", num.trees = 10)

part = partition(task, ratio = 0.8) # by default stratifies on the target column

learner$train(task, row_ids = part$train)
preds = learner$predict(task, row_ids = part$test)
preds$score(msr("classif.ce"))
```

## `mlr3proba`: Basics {-}

:::{.callout-tip title="Teaching Aims"}
- Understand survival tasks and how they differ from regression/classification
- Know how to conduct basic modeling with `{mlr3proba}`
- Prediction types
- Survival measures
:::

`{mlr3proba}` extends `{mlr3}` with survival analysis capabilities.

:::{.callout-important}
As of now, `{mlr3proba}` is not on CRAN, but you can install it [from GitHub](https://github.com/mlr-org/mlr3proba/?tab=readme-ov-file#installation) or [r-universe](https://mlr-org.r-universe.dev/mlr3proba).
More info is also available on the respective [mlr3 book chapter](https://mlr3book.mlr-org.com/chapters/chapter13/beyond_regression_and_classification.html#sec-survival).
:::

### Survival Tasks {-}

We'll start by using the built-in `lung` dataset, which is a survival task with $7$ features and $168$ observations:

```{r}
library(mlr3proba)
task = tsk("lung")

task
```

[See online reference](https://mlr3proba.mlr-org.com/reference/TaskSurv.html#methods) to useful methods offered by the main `TaskSurv` class.
Some examples:

Target `Surv` object from `{survival}` (`+` denotes censored observation):

```{r}
head(task$truth())
```

Proportion of censored observations:

```{r}
task$cens_prop()
```

Does the data satisfy the **proportional hazards** assumption? Get the p-value from the Grambsch-Therneau test (see `?survival::cox.zph`):

```{r}
task$prop_haz() # barely, p > 0.05 => PH
```

Using the `autoplot()` function from `{ggplot2}`, we get the Kaplan-Meier curve:

```{r}
library(ggplot2)
autoplot(task) +
  labs(title = "Lung dataset: Kaplan-Meier curve")
```

Tasks shipped with `{mlr3proba}`:

```{r}
as.data.table(mlr_tasks)[task_type == "surv", list(key, label, nrow, ncol)]
```

:::{.callout-note}
- Use [as_task_surv()](https://mlr3proba.mlr-org.com/reference/as_task_surv.html) to convert your own datasets to a `TaskSurv` object
- Try `tsk("lung")$help()` to get more info about the dataset and pre-processing applied
:::

### CoxPH learner {-}

The classical Cox Proportional Hazards model:
```{r}
cox = lrn("surv.coxph")
cox
```

Train the cox model and access the fit object from the `{survival}` package:
```{r}
set.seed(42)
part = partition(task, ratio = 0.8) # by default, stratification is on `status` variable
cox$train(task, row_ids = part$train)

cox$model
```

Visual output of the model, using the latest version from Github of `{mlr3viz}`:

```{r}
autoplot(cox)
```

### Prediction types {-}

Let's predict using the trained cox model on the test set (output is a [PredictionSurv](https://mlr3proba.mlr-org.com/reference/PredictionSurv.html) object):

```{r}
p = cox$predict(task, row_ids = part$test)
p
```

:::{.callout-tip title="Prediction types in mlr3proba"}
- `crank`: Continuous risk ranking
- `lp`: Linear predictor calculated as $\hat\beta * X_{test}$
- `distr`: Predicted survival distribution, either discrete or continuous
- `response`: Predicted survival time
:::

For the cox model, `crank = lp` (the higher, the more risk):

```{r}
p$lp
```

Survival prediction is a 2D `matrix` essentially, with dimensions: *observations* x *time points*:

```{r}
p$data$distr[1:5, 1:5]
```

Users should use the [distr6](https://github.com/xoopR/distr6) interface to access this prediction type, which allows us to retrieve survival probabilities (or hazards) for any time point of interest:

```{r}
# first 4 patients in the test set, specific time points:
p$distr[1:4]$survival(c(100, 500, 1200))
```

Visualization of predicted survival curves for $3$ test patients:

```{r}
p2 = p$clone()$filter(row_ids = c(1,24,40))
autoplot(p2, type = "preds")
```

### Model evaluation {-}

:::{.callout-tip title="Model validation"}
Validation of a survival model can be done by assessing:

1. **Discrimination**: the ability of the model to distinguish between low and high risk patients
2. **Calibration**: the agreement between the observed and predicted survival probabilities
3. **Overall performance**: the distance between the observed and predicted survival probabilities
:::

Many measures included in `mlr3proba`:
```{r}
mlr_measures$keys(pattern = "surv")
```

Most commonly used metrics are for assessing discrimination, such as **Harrell's C-index**, **Uno's C-index** and the **(time-dependent) AUC**:
```{r}
harrell_c = msr("surv.cindex", id = "surv.cindex.harrell")
uno_c = msr("surv.cindex", weight_meth = "G2", id = "surv.cindex.uno")
uno_auci = msr("surv.uno_auc", integrated = TRUE) # across all times in the test set
uno_auc = msr("surv.uno_auc", integrated = FALSE, times = 10) # at a specific time-point of interest

harrell_c
uno_auc
```

:::{.callout-note}
- Not all measures are applicable to all models - **prediction type** matters!
- Most discrimination metrics use the `crank` or `lp` prediction
:::

```{r}
p$score(harrell_c)
p$score(uno_c, task = task, train_set = part$train)
```

Calibration is traditionally performed graphically via calibration plots:
```{r calib-plot-}
autoplot(p, type = "calib", task = task, row_ids = part$test)
```

But there exists also calibration metrics, e.g. **D-Calibration**:
```{r}
dcal = msr("surv.dcalib")
dcal

p$score(dcal)
```

Overall survival prediction performance can be assessed by scoring rules such as the **Integrated Survival Brier Score** (ISBS) and the **Right-censored Log-Loss** (RCLL) among others:
```{r}
rcll = msr("surv.rcll")
rcll

p$score(rcll)
```

```{r}
ibrier = msr("surv.brier", proper = TRUE)
ibrier

p$score(ibrier, task = task, train_set = part$train)
```

### Using ML survival models {-}

Mention from `mlr3extralearners`: cv.glmnet, aorsf, ranger, CoxBoost

## Benchmark example: GEX + clinical data {-}

- From the tutorial (https://ocbe-uio.github.io/survomics/survomics.html#workflow)
- data => https://github.com/ocbe-uio/survomics/blob/main/data.rds
- Benchmark + posthoc analysis of results with `mlr3benchmark`
