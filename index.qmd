---
title: "Intro to Machine Learning for Survival Analysis with mlr3"
author: "[John Zobolas](https://github.com/bblodfon), [Lukas Burk](https://lukasburk.de/)"
date: last-modified
description: "Tutorial for the useR! 2024 conference in Salzburg, Austria (8-11 July)"
bibliography: references.bib
format:
  html:
    date: last-modified
    code-block-bg: true
    code-copy: true
    code-fold: show
    code-overflow: wrap
    code-block-border-left: true
    toc: true
    toc-location: left
    html-math-method: katex
    page-layout: full
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = FALSE)
```

## `mlr3`: Basics {-}

:::{.callout-tip title="Teaching Aims"}
- Understand how `{mlr3}` is structured [@mlr3]
- Access `learner`s and (built-in) `task`s
:::

To get started, we load `{mlr3verse}`, which will load various packages from the `{mlr3}` ecosystem:

```{r, message=FALSE, warning=FALSE}
library(mlr3verse)
```

`{mlr3}` ships with wrappers for many commonly used machine learning algorithms ("learners").  
We can access the list of available learners using the `mlr_learners` dictionary:
```{r}
sample(mlr_learners$keys(), 10)
```

One example:
```{r}
lrn("classif.ranger")
```

:::{.callout-note}
Use `lrn("classif.ranger")$help()` to view the help page, with links to documentation for parameters and other information about the wrapped learner.
:::

Built-in tasks can be accessed using the `mlr_tasks` dictionary:
```{r}
head(as.data.table(mlr_tasks)[, list(key, label, task_type, nrow, ncol, properties)])
```

One example:

```{r}
tsk("penguins_simple")
```

:::{.callout-note}
Tasks encapsulate a data source (typically a `data.table`) and additional information regarding which variables are considered features and target.
Tasks can also specify additional properties such as stratification, which we will see later.
:::

## Example: Train-Predict-Evaluate {-}

:::{.callout-tip title="Teaching Aims"}
- Perform a simple train-predict-evaluate step
- Use built-in classification `task` and `learner`
:::

The below code snippet trains a random forest model on the `penguins_simple` task (a simplified version of the `palmerpenguins` dataset, but without missing values) and evaluates the model's performance using the classification error metric:

```{r fit-penguins}
task = tsk("penguins_simple")
learner = lrn("classif.ranger", num.trees = 10)

part = partition(task, ratio = 0.8) # by default stratifies on the target column

learner$train(task, row_ids = part$train)
preds = learner$predict(task, row_ids = part$test)
preds$score(msr("classif.ce"))
```

Learn more by reading the [respective chapter](https://mlr3book.mlr-org.com/data_and_basic_modeling.html) on the `mlr3` book.

## `mlr3proba`: Basics {-}

:::{.callout-tip title="Teaching Aims"}
- Understand survival tasks and how they differ from regression/classification
- Know how to conduct basic modeling with `{mlr3proba}` [@mlr3proba]
- Prediction types
- Survival measures
:::

`{mlr3proba}` extends `{mlr3}` with survival analysis capabilities.

:::{.callout-important}
As of now, `{mlr3proba}` is not on CRAN, but you can install it [from GitHub](https://github.com/mlr-org/mlr3proba/?tab=readme-ov-file#installation) or [r-universe](https://mlr-org.r-universe.dev/mlr3proba).
More info is also available on the respective [mlr3 book chapter](https://mlr3book.mlr-org.com/chapters/chapter13/beyond_regression_and_classification.html#sec-survival).
:::

### Survival Tasks {-}

We'll start by using the built-in `lung` dataset, which is a survival task with $7$ features and $168$ observations:

```{r}
library(mlr3proba)
task = tsk("lung")

task
```

[See online reference](https://mlr3proba.mlr-org.com/reference/TaskSurv.html#methods) to useful methods offered by the main `TaskSurv` class.
Some examples:

Target `Surv` object from `{survival}` (`+` denotes censored observation):

```{r}
head(task$truth())
```

Proportion of censored observations:

```{r}
task$cens_prop()
```

Does the data satisfy the **proportional hazards** assumption? Get the p-value from the Grambsch-Therneau test (see `?survival::cox.zph` [@Grambsch1994]):

```{r}
task$prop_haz() # barely, p > 0.05 => PH
```

Using the `autoplot()` function from `{ggplot2}`, we get the Kaplan-Meier curve:

```{r km-curve}
library(ggplot2)
autoplot(task) +
  labs(title = "Lung dataset: Kaplan-Meier curve")
```

Tasks shipped with `{mlr3proba}`:

```{r}
as.data.table(mlr_tasks)[task_type == "surv", list(key, label, nrow, ncol)]
```

:::{.callout-note}
- Use [as_task_surv()](https://mlr3proba.mlr-org.com/reference/as_task_surv.html) to convert your own datasets to a `TaskSurv` object
- Try `tsk("lung")$help()` to get more info about the dataset and pre-processing applied
:::

### CoxPH learner {-}

The classical Cox Proportional Hazards model:
```{r}
cox = lrn("surv.coxph")
cox
```

Train the cox model and access the fit object from the `{survival}` package:
```{r}
set.seed(42)
part = partition(task, ratio = 0.8) # by default, stratification is on `status` variable
cox$train(task, row_ids = part$train)

cox$model
```

Visual output of the model, using the latest version from Github of `{mlr3viz}`:

```{r cox-model-viz}
autoplot(cox)
```

### Prediction types {-}

Let's predict using the trained cox model on the test set (output is a [PredictionSurv](https://mlr3proba.mlr-org.com/reference/PredictionSurv.html) object):

```{r}
p = cox$predict(task, row_ids = part$test)
p
```

:::{.callout-tip title="Prediction types in mlr3proba"}
- `crank`: Continuous risk ranking
- `lp`: Linear predictor calculated as $\hat\beta * X_{test}$
- `distr`: Predicted survival distribution, either discrete or continuous
- `response`: Predicted survival time
:::

For the cox model, `crank = lp` (the higher, the more risk):

```{r}
p$lp
```

Survival prediction is a 2D `matrix` essentially, with dimensions: *observations* x *time points*:

```{r}
p$data$distr[1:5, 1:5]
```

Users should use the [distr6](https://github.com/xoopR/distr6) interface [@distr6] to access this prediction type, which allows us to retrieve survival probabilities (or hazards) for any time point of interest:

```{r}
# first 4 patients in the test set, specific time points:
p$distr[1:4]$survival(c(100, 500, 1200))
```

Visualization of predicted survival curves for $3$ test patients:

```{r pred-curves}
p2 = p$clone()$filter(row_ids = c(1,24,40))
autoplot(p2, type = "preds")
```

### Model evaluation {-}

:::{.callout-tip title="Model validation"}
Validation of a survival model can be done by assessing:

1. **Discrimination**: the ability of the model to distinguish between low and high risk patients
2. **Calibration**: the agreement between the observed and predicted survival probabilities
3. **Overall performance**: the distance between the observed and predicted survival probabilities
:::

Many measures included in `mlr3proba`:

```{r}
mlr_measures$keys(pattern = "surv")
```

Most commonly used metrics are for assessing discrimination, such as **Harrell's C-index** [@Harrell1982], **Uno's C-index** [@Uno2011] and the **(time-dependent) AUC** [@Heagerty2005; @Uno2007]:

```{r}
harrell_c = msr("surv.cindex", id = "surv.cindex.harrell")
uno_c = msr("surv.cindex", weight_meth = "G2", id = "surv.cindex.uno")
uno_auci = msr("surv.uno_auc", integrated = TRUE) # across all times in the test set
uno_auc = msr("surv.uno_auc", integrated = FALSE, times = 10) # at a specific time-point of interest

harrell_c
uno_auc
```

:::{.callout-note}
- Not all measures are applicable to all models - **prediction type** matters!
- Most discrimination metrics use the `crank` or `lp` prediction
:::

```{r}
p$score(harrell_c)
p$score(uno_c, task = task, train_set = part$train)
```

Calibration is traditionally performed graphically via calibration plots:
```{r calib-plot}
autoplot(p, type = "calib", task = task, row_ids = part$test)
```

But there exists also calibration metrics, e.g. **D-Calibration** [@Haider2020]:
```{r}
dcal = msr("surv.dcalib")
dcal

p$score(dcal)
```

Overall survival prediction performance can be assessed by scoring rules such as the **Integrated Survival Brier Score** (ISBS) [@Graf1999] and the **Right-censored Log-Loss** (RCLL) [@Avati2020] among others:
```{r}
rcll = msr("surv.rcll")
rcll

p$score(rcll)
```

```{r}
ibrier = msr("surv.brier", proper = TRUE)
ibrier

p$score(ibrier, task = task, train_set = part$train)
```

## Using and tuning ML survival models on high-dimensional data {-}

:::{.callout-tip title="Teaching Aims"}
- Create your own survival tasks from external data
- Evaluate performance using `resample()`
- Tune hyperparameters with `auto_tuner()`
:::

So far we have used the Cox regression model, but there are many more machine learning methods available via `mlr3extralearners` ([learner list](https://mlr-org.com/learners.html))!
We will take a look at the following:

- Cox elastic net via [`glmnet`](https://glmnet.stanford.edu/articles/Coxnet.html) [@Friedman2010]
  - We will use `lrn("surv.cv_glmnet")`, wich internally tunes for `lambda` using cross-validation
- Likelihood-based boosting via [`CoxBoost`](https://github.com/binderh/CoxBoost) [@Binder2008]
  - We later use `lrn("surv.cv_coxboost", penalty = "optimCoxBoostPenalty")`, which also uses internal cross-validation to tune its parameters
- Random Forests via [`ranger`](https://imbs-hl.github.io/ranger/) [@Ishwaran2008]
- Oblique Random Forests via [`aorsf`](https://docs.ropensci.org/aorsf/) [@Jaeger2023]

These learners then cover the range from penalized regression to tree ensembles and boosting.

Let's take these learners for a spin on a subset of TCGA breast cancer data with gene expression and clinical features.
We first need to create a `TaskSurv` object from the data, which we can do by reading in the data and then using `as_task_surv()`.
We also add the `status` column to the stratum, which is necessary for the resampling to ensure a similar proportion of events in the resampling folds with the complete dataset.


```{r tcga-task}
tcga = readRDS("data/tcga.rds")

task_tcga = mlr3proba::as_task_surv(
  x = tcga, 
  time = "time", event = "status", id = "BRCA-TCGA"
)

# Set stratum for resampling
task_tcga$set_col_roles("status", add_to = "stratum")
task_tcga
```

We can instantiate our learners as we've seen before --- we're sticking to mostly vanilla settings for now.  
We can let `glmnet` determine the optimal value for `lambda` with it's internal cross-validation method
Similarly, `CoxBoost` could tune itself, but we'll stick with a simple version to save some time on compute!
For the forests, we use 100 trees each for speed and otherwise accept the defaults.

:::{.callout-note}
To speed things up a little, we let learners use 4 parallel threads (`num.threads` in `ranger` and `n_thread` in `aorsf`), which you may want to change depending on your available resources!
:::

```{r lrns-init}
lrn_glmnet = lrn("surv.cv_glmnet", alpha = 0.5)
lrn_coxboost = lrn("surv.coxboost", penalty = 100)
lrn_ranger = lrn("surv.ranger", num.trees = 100, num.threads = 4)
lrn_aorsf = lrn("surv.aorsf", n_tree = 100, n_thread = 4)
```

We can now use `resample()` to evaluate the performance of each of these learners on the task.
To do this, we decide on two measures: Harrell's C and the integrated brier score, and we also instantiate a resampling to use for comparison, such that we ensure all learners see the same data.

```{r rr-glmnet}
measures = list(msr("surv.cindex", id = "cindex"), msr("surv.brier", id = "ibs"))

resampling = rsmp("cv", folds = 3)
resampling$instantiate(task_tcga)

rr_glmnet = resample(
  task = task_tcga,
  learner = lrn_glmnet,
  resampling = resampling
)

rr_glmnet$score(measures)
```

Well, looks like `glmnet` out of the box does not do well on this dataset, judging by the C-index of 0.5.
This is what the null model achieves, after all!

Feel free to play with the parameters of `glmnet` a bit more --- for example, does changing `alpha` help?

We can repeat the same procedure for the other learners:

```{r rr-others}
rr_coxboost = resample(
  task = task_tcga,
  learner = lrn_coxboost,
  resampling = resampling
)

rr_ranger = resample(
  task = task_tcga,
  learner = lrn_ranger,
  resampling = resampling
)

rr_aorsf = resample(
  task = task_tcga,
  learner = lrn_aorsf,
  resampling = resampling
)

rr_coxboost$score(measures)
rr_ranger$score(measures)
rr_aorsf$score(measures)
```

Now we have a comparison of the performance of the different learners on the task.
We can again aggregate these results to get a summary of the performance of each learner across all resamplings:

```{r}
rr_glmnet$aggregate(measures)
rr_coxboost$aggregate(measures)
rr_ranger$aggregate(measures)
rr_aorsf$aggregate(measures)
```

Of course in practice we want to tune these learners for optimal performance.
Tuning can be quite a complex topic, but `mlr3` makes it relatively simple with the `auto_tuner` approach.
Without going into too much detail about the theory, for tuning we need:

- A learner to tune, with information on which parameters to tune in which range (the search space)
- A strategy (`tuner`), such as random search, grid search, or more advanced options, which defines how we search for new parameter values to try
- A resampling strategy to evaluate performance during tuning
- A tuning measure top optimize for
- A stopping criterion, e.g. stopping after 100 evaluations


```{r glmnet-autotune}
at_glmnet = auto_tuner(
  learner = lrn("surv.cv_glmnet", alpha = to_tune(0, 1)),
  tuner = tnr("grid_search"),
  resampling = rsmp("cv", folds = 3),
  measure = msr("surv.cindex"),
  term_evals = 100
)
```

We can then try this out on the full dataset like so:

```{r glmnet-at-train}
at_glmnet$train(task_tcga)
```

We can see all evaluated parameter combinations in the `$tuning_instance` and the best result in `$tuning_result`

```{r}
at_glmnet$tuning_instance
at_glmnet$tuning_result
```


We get a better result than 0.5 now, but note *we used all of the data* now, so while this would be the approach we could use to find a model for new data, right now we want to compare our learners fairly!
That means: *Nested resampling*, where we use resampling for tuning, and for evaluation in two layers.

In this step, we simplify the `resample()` steps using `benchmark()`, and we'll also tune some of the learners.

We start by first defining our learners with tuning, using `cv_coxboost` to tune itself without the `auto_tuner` and tuning `glmnet` similar to before, but we'll set small budgets to keep the runtime of this example code managable:

```{r}
at_glmnet = auto_tuner(
  learner = lrn("surv.cv_glmnet", alpha = to_tune(0, 1)),
  tuner = tnr("grid_search"),
  resampling = rsmp("cv", folds = 3),
  measure = msr("surv.cindex"),
  term_evals = 25
)

# For CoxBoost's optimCoxBoostPenalty, iter.max is analogous to term_evals 
lrn_cvcoxboost = lrn("surv.cv_coxboost", penalty = "optimCoxBoostPenalty", iter.max = 10)
```

The we create a *benchmark design* of one or more tasks and at least two learners like so:

```{r}
design = benchmark_grid(
  tasks = task_tcga,
  learners = list(at_glmnet, lrn_ranger, lrn_aorsf, lrn_cvcoxboost),
  resamplings = resampling
)

design
```

To perform the benchmark, we use the aptly named `benchmark()` function, which will perform the necessary resampling iterations and store the results for us --- this will take a moment!

```{r bm-tune, echo=FALSE, eval=interactive()}
bmr = benchmark(design, store_models = TRUE, store_backends = TRUE)
bmr
```

When we `$score()` or `$aggregate()` the benchmark result, we should get the same exact scores as before for the untuned learners because we used the instantiated resampling from earlier, meaning each learner again saw the same data --- `glmnet` and `coxboost` however should do better because now we spent time tuning them!

```{r bmr-aggr, eval=interactive()}
# bmr$score(measures)
bmr$aggregate(msr("surv.cindex"))
```

We can also visualize the results --- see `?autoplot.BenchmarkResult` for more options:

```{r bmr-box, eval=interactive()}
autoplot(bmr, type = "boxplot", measure = msr("surv.cindex"))
```

From our quick tests, which learner now seems to have done the best?
Given that we used these learners more or less off the shelf without tuning, we should not put too much weight on these results, but it's a good starting point for further exploration!

Tuning is a complex topic we barely scratched the surface of, but you can learn more about it in the [mlr3book chapter](https://mlr3book.mlr-org.com/chapters/chapter4/hyperparameter_optimization.html)!

## Benchmarking with multiple datasets {-}

:::{.callout-tip title="Teaching Aims"}
- Perform a small-scale benchmark
- Aggregate and visualize the results
- Perform a statistical analysis of the results
:::

A proper benchmark can take a lot of time and planning, but it can pay off to get a good overview of the performance of different learners on different tasks relevant to your field!

In this example, we'll take a number of small datasets provided by `mlr3proba` and benchmark the learners we used before on them.
These tasks are small enough to hopefully not spend too much time waiting for computations to finish, but we hope you get enough of an idea to feel confident to perform your own experiments!

The procedure is as follows:

1. Gather tasks as a list.
2. Gather our learners. Normally this would include deciding on tuning spaces!
3. Define a resampling strategy.
4. Decide on measures to use.


For step 1, we'll select some survival tasks from `mlr_tasks` for this benchmark:

```{r tasks-bm}
tasks = list(
  tsk("actg"),
  tsk("gbcs"),
  tsk("grace"),
  tsk("lung"),
  tsk("mgus")
)

tasks
```

Many have categorical features (`fct`), which can be a bit tricky to handle for some learners, so we will take a shortcut and add a feature encoding `PipeOp` to the learners that need it.
Pipelines and preprocessing are very useful, and [the mlr3book](https://mlr3book.mlr-org.com/chapters/chapter9/preprocessing.html#factor-encoding) again has you covered!

We use the `po("encode")` pipe operator to encode the factors as dummy-encoded variables (`method = "treatment"`) for the Cox model, and use the default (one-hot encoding) for the others.
The `%>>%` operator is used to chain `PipeOps` and learners together, and we wrap the pipeline in `as_learner` such that we can treat it as a learner just like the others.


```{r lrn-bm}
preproc = po("encode", method = "treatment")

learners = list(
  cox = as_learner(preproc %>>% lrn("surv.coxph",id = "cph")),
  glmnet = as_learner(preproc %>>% lrn("surv.cv_glmnet", alpha = 0.5)),
  ranger = lrn("surv.ranger", num.trees = 100, num.threads = 4),
  aorsf = lrn("surv.aorsf", n_tree = 100, n_thread = 4),
  coxboost = as_learner(preproc %>>% lrn("surv.coxboost", penalty = 100))
)
```

A small convenience thing we can do here is to set IDs for the learners, which will make the output of further steps more readable:

```{r, echo=FALSE}
mlr3misc::imap(learners, function(l, id) l$id = id)
```

Moving on to the benchmark, we create a design grid as before, only now we have multiple tasks.
Luckily, `benchmark_grid()` can handle this for us by instantiating the resampling for each task, so we don't have to worry about this here!

```{r}
design = benchmark_grid(
  tasks = tasks,
  learners = learners,
  resamplings = rsmp("cv", folds = 3)
)

design
```


```{r bm, echo=FALSE}
# This might take a moment!
bmr = benchmark(design, store_models = TRUE, store_backends = TRUE)
```

We pick the IBS again and aggregate the results:

```{r}
measure = msr("surv.brier", id = "ibs")

bmr$aggregate(measure)
```

### Statistical analysis {-}

Rather than just computing average scores, we can leverage `mlr3benchmark` for additional analysis steps, including a statistical analysis of the results.
The starting point is to convert the benchmark result (`bmr`) to an aggregated benchmark result (`bma`), which is a more convenient format for further analysis:

```{r as-bma}
library(mlr3benchmark)
bma = as_benchmark_aggr(bmr, meas = measure)
bma
```

This brings with it a few more `autoplot` methods, see `?autoplot.BenchmarkAggr`.

```{r bma-box}
autoplot(bma, type = "box", meas = "ibs")
```

For the statistical analysis, we can use a simple rank-based analysis following  @Demsar2006 with a global Friedman test to see if there are significant differences between the learners:

```{r}
bma$friedman_test()
```

The corresponding post-hoc test for all pairwise comparison can be performed as follows:

```{r}
bma$friedman_posthoc()
```

A visual approach is the critical difference plot [@Demsar2006], which shows a connecting line between the learners that _are not_ statistically different from each other (as far as their average ranks are concerned).

```{r critical-difference-ibs}
autoplot(bma, type = "cd", meas = "ibs", ratio = .7)
```


## Conclusion

We have conducted a tiny benchmark experiment on a few survival tasks using a few learners --- a good starting point for further exploration!
Advanced topics we did not cover in more detail include tuning and more advanced pipelines, but we hope you got a good overview of the capabilities of `mlr3proba` and `mlr3` in general.

## Session Info

```{r sess}
sessioninfo::session_info()
```

